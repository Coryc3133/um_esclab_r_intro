---
title: "R Bootcamp Day 2: Exploring the tidyverse"
author: "Cory Costello"
date: "9/26/2019"
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
---

# Goals 

By the end of the day, you will:

+ Learn how to read in and examining Data
+ Begin understanding tidyverse "grammar"
+ Begin understanding the grammar of data manipulation, `dplyr` 
+ What a pipe is and how to use pipes to chain together `tidyverse` verbs


# 0. Quick Refresher & Warm up 

Before we get started with new content, let's do a quick review of what we covered last time. 

## Exercise 0.1a
> Create a vector, x, and make it the numbers 1 throu 20. Change the 3rd and fourth element so that they are 5 times greater.

```{r}
```

Last time, we ended by installing two packages, the `rio` package and the `tidyverse` package. This time, we're going to learn how to use them to get our data into R, manipulate it, and visualize it. We'll load them both now:

```{r}
library(tidyverse)
library(rio)
```


# 1. Reading data into R

Now that we have the `rio` package and a basic idea of how packages work, let's use it to read in some data! If you're used to a GUI system like excel or SPSS, reading in data in R can be a little bit confusing at first. 

Reading in data generally has two slightly challenging aspects for new users:

1. You need to call a function that works with a particular data format (csv, txt, sav, etc.).

2. You need to tell R where to look.

We'll use `import()` from `rio`, which does the first part for us. We just call `import()` and it calls the right read function given the file's extension (`.csv`, `.txt`, `.sav`, `.xlsx`, etc.). 

## 1.1 Working Directories & File Paths 

On to challeng # 2. When R looks for a file, it has a starting point. This is called the *working directory*. The working directory that you're currently in is displayed in the console window and the files tab. You can also get it with the `getwd()` function:

```{r}
getwd()
```

For this tutorial, your working directory should be wherever you downloaded the materials and opened the r project. If you opened the .Rmd, you should be in the uoregon_r_bootcamp directory already. You can change your working directory with:

```{r eval = FALSE}
setwd("PATH")
```

It is considered bad practice to use `setwd()` and it typically will not work in an Rmd; you can see the author's rationale for that [here](https://groups.google.com/forum/#!topic/knitr/knM0VWoexT0).

My recommondation is to either:

1. Use an R project. This is a relatively foolproof way of doing things.    
2. Use .Rmds and always open Rstudio from the particular Rmd. This is a little less foolproof.
3. Use .Rmds and set the working directory in the console or through R Studio's GUI. This is the least foolproof.

1 is probably best practice. 2 can save some time but may not be worth it. 3 is too risky for my tastes. If you do need to set the working directory from R Studio's GUI, do the following:

Session > Set Working Directory > Choose Directory

You can choose the folder you want to work in. The code for setting the working directory will populate in the console. You can then copy/paste this into your code if you'd like.

## 1.2 Importing & Exporting data with rio

### 1.2.1 Importing Data
As I mentioned earlier, the `import()` function from `rio` really simplifies reading data into R. Let's see that first hand by reading in the `pragmatic_scales_data`, a csv file:

```{r}
library(rio)
ps_data <- import("pragmatic_scales_data.csv")
```

Let's say that `ps_data` were a `.sav` SPSS data file. In rio, this is no problem, it will call the right function to read in `.sav` files. Let's give it a try, reading in the `pragmatic_scales_data.csv` located in the `data` subdirectory of our current working directory:

```{r}
ps_data <- import("data/ps_data.sav")
```

Notice that all I had to change was where to look (telling it to go to the `data/` subdirectory and the file called `ps_data.csv`)

You can also import data from a website. For example, this dataset is also hosted on github, so we could download it from there using `import()` too:

```{r}
ps_data <- import("https://raw.githubusercontent.com/Coryc3133/uoregon_r_bootcamp/master/pragmatic_scales_data.csv")
```

You can see all of the file formats `rio` works with by running `?import`.

### 1.2.2 Exporting Data

You can also use rio to export your data, saving it in any of the formats that it works with. This is really simple and works just like `import()`, but is called `export()`. For `export()`, you provide the R dataframe object you want to export, and the path/name for the new file. For example, let's say I want to export `ps_data` as an xlsx file and put it into the `/data` subdirectory. I could do that with export:

```{r}
export(ps_data,
       "data/ps_data.xlsx")
```

### Exercise 1.2a
> I made a mistake when creating this and left the datasets in the `uoregon_r_bootcamp` folder instead of putting them into the `/data` directory. Let's fix that. We already fixed ps_data, but now I want you to fix `another_data_set.csv`. First import the data as `another_df`:

```{r}

```


### Exercise 1.2b
> Now I want you to export the data and save it into the `data/` directory. Make sure the name of the dataframe is `another_data_set`, and make sure you save it out as a csv.

```{r}

```

### Exercise 1.2c
> One of my colleagues insists we send them a .sav file so that they can run the analyses in SPSS. Make another copy of `another_data_set` in the `data/` subdirectory that is in the .sav format. 

```{r}
```

### Exercise 1.2d
> Finally, let's read one of these datasets to make sure everything worked as expected. Import the .sav version of another_data_set as `another_df`.

```{r}

```

## 1.3 Examining Your Data

Now that your data is in R, you may want to take a look at it. There are a few different ways to do that, which each offer different information.

### 4.3.1 View
One way is to click on the View button in the environment pane. You should see ps_data in the environment pane with a little data icon at the far right. Click on that icon. You'll notice that this ran `View(ps_data)` in the console. We can do that with code:

```{r eval = FALSE}
View(ps_data)
```

Note that the V in `View()` is capital!.

### 1.3.2 head and tail

You can also see just the first few rows of a dataframe with `head()`:

```{r}
head(ps_data)
```

This can be useful when you have very large datasets as it is much faster than the View function. `head()` prints 6 rows by default, but you can increase or decrease that with the `n = ` argument. For example, imagine we want to see the first 20 rows:

```{r}
head(ps_data, n = 20)
```

tail is the complement to head, displaying just the final rows from a dataframe:

```{r}
tail(ps_data)
```

### 1.3.3 Examine Structure with str

We saw str a little earlier when we first introduced dataframes. It's worth mentioning it again because it can be so useful when you import data to see how the read function interpreted the variables. Let's see:

```{r}
str(ps_data)
```

### 1.3.4 Summary

The `summary()` funciton can be used to get a quick sense of each of the variables in a dataframe. It displays summary information for each variable. It displays different kinds of information depending on the variable's type.

```{r}
summary(ps_data)
```

# 2. More Advanced Indexing and Modifying a data frame in base R.

## 2.1 Indexing

Let's start by reviewing indexing dataframes.

### 2.1.1 Bracket Indexing with Numerical Indices and Names

Recall that uou can select entries in the data frame just like indexing a matrix, i.e., [row, column]

```{r}
ps_data[1, 5]
ps_data[1, "condition"]
```

And you can get a whole row or column by leaving the other dimension empty. Let's get all rows of condition:

```{r}
ps_data[, "condition"]
```
### 2.1.2 Indexing with $

Recall that you can also get a column from a df by using `df$column`. For example, we could get condition:

```{r}
ps_data$condition
```

## 2.1.3 Indexing with Logical Tests

Up to this point, we've only covered indexing by numerical index or name. But, you can also index via logical tests. To do this in base R, we use the `which()` function, which returns the indices where a condition is true. We can test if things are equal, not equal, greater, or lesser using the following symbols:

Test  | symbol
------|-------
Equal | ==
Not equal | !=
Greater than | >
Lesser than | <
Greater than or Equal to | >=
Lesser than or Equal to | <=

Let's put this to use and get all of the indices where condition is equal to label:

```{r}
which(ps_data$condition == "Label")
```

We can combine this with `[]` indexing to do even more powerful subsetting. For example, we can put this `which()` call within the row position to extract the rows for subjects in the "Label" condition. 

```{r}
ps_data[which(ps_data$condition == "Label"),]
```

Or, we could get all of the rows where subjects are greater than or equal to 2.5 years old:

```{r}
ps_data[which(ps_data$age >= 2.5),]

```

You can also use logical tests for columns, though that is a little trickier. Let's get all of the columns that start with the letter c. We can look for variables that start with c by using `str_detect()` on the column names, looking for entries that start with c `"^c"`.

```{r}
ps_data[,grep("^c", colnames(ps_data))]
```

You can also do more complicated logical tests by including `&` for AND and `|` for OR. For example, let's get subjects that were in the label condition and less than 3 years old:

```{r}
ps_data[which(ps_data$condition == "Label" & ps_data$age < 3),]
```

Or we might want subjects rows where the item is either faces or houses.

```{r}
ps_data[which(ps_data$item == "faces" | ps_data$item == "houses"),]
```


### Exercise 2.1a
> Get the first 10 rows of the item column from the `ps_data` df.

```{r}

```

### Exercise 2.1b
> Using logical indexing, get all of the rows where age is greater than or equal to 3.5 and item equals "faces".

```{r}

```


### Exercise 2.1c (*Bonus*)
> Using logical indexing, get all of the columns that start with either s or a (Hint: you only need 1 grep call and | can be used within a string).

```{r}
```

# 3. Introduction to the `tidyverse`

We installed and loaded the `tidyverse` earlier and now we'll learn some of the basics. ["The `tidyverse` is an opionated collection of R packages designed for data science"](https://www.tidyverse.org/). It's a suite of packages designed with a consistent philosophy and aesthetic. This is nice because all of the packages are designed to work well together, providing a consistent framework to do many of the most common tasks in R including:

* data cleaning (`tidyr`)
* data manipulating (`dplyr`)
* data visualization (`ggplot2`)
* working with strings (`stringr`)
* working with factors (`forcats`)

Among others. We'll be using functions from each of these packages today and tomorrow.

Today we'll just focus on data manipulation with `dplyr` and data visualization with `ggplot2`

Three qualities of the `tidyverse` are worth mentioning at the outset:

1. packages are designed to be like *grammars* for their task, so we'll be using terms like verbs to discuss the tidyverse. The idea is that you can string these grammatical elements together to form more complex statements, just like with language. 

2. The first argument of (basically) every function is data. This is very handy, especially when it comes to piping (discussed below).

3. Variable names are *usually* not quoted.

The last thing I want to be sure to mention is that the tidyverse packages all have [helpful cheatsheets](https://rstudio.com/resources/cheatsheets/). I think these are one of the handiest R resources out there, and I look at them regularly.

Without further ado, let's get started with some basic use of `dplyr`:

## 3.1 `dplyr`

[`dplyr`](https://dplyr.tidyverse.org/) is a grammar of data manipulation. It is made up of several verbs for common data manipulation tasks.

### 3.1.1 Selecting Columns

The `select()` is the first verb we'll cover and is how we can subset columns. If you're like me, you'll soon find it **much** easier to use than the bracket subsetting we did earlier. 

`select()` is the verb for selecting columns from a dataframe. The first argument is data followed which columns you would like to select.

#### 3.1.1.1 Basics of Select

You can indicate the columns you want to select using unquoted names. For example, let's select just `age` from `ps_data`

```{r}
select(ps_data, age)
```

You can select more columns by enetring them, separated by a comma. Let's get age and condition:

```{r}
select(ps_data, age, condition)
```

You can also use columns' positions. We could get `subid`, the first column, by supplying a 1:

```{r}
select(ps_data, 1)
```

Or, you can say which variable you don't want by prefacing its name or index with a `-`. For example, let's get rid of age.

```{r}
select(ps_data, -age)
```

You could also get rid of by referencing its index:

```{r}
select(ps_data, -5)
```

You can also use `:` to select or de-select a range of variables. This can be done with reference to their numerical index:

```{r}
# select first three:
select(ps_data, 1:3)

# de-select last three:
select(ps_data, -(1:3)) # - requires parenthetical sequence
```

And you can even use ranges of variable names. 
```{r}
# select first three
select(ps_data, subid:correct)

# deselect first three
select(ps_data, -(subid:correct))
```

#### 3.1.1.2 Helper functions

The best part of select is that it has special helper function to perform common kinds of selection tasks.

##### starts_with
For example, let's say we want all the variables that start with 'c'. We can use the `starts_with()` helper function:

```{r}
select(ps_data, starts_with("c"))
```

That is way simpler than the base R solution we discussed above, which was

```{r}
ps_data[, grep("^c", colnames(ps_data))]
```

#### ends_with
Select columns that end with some character:
```{r}
select(ps_data, ends_with("e"))
```

#### contains
Select columns that contain a character.
```{r}
select(ps_data, contains("i"))
```

There are others too, but these are the most common. Here is a table of all of them.

function | what it does
---------|-------------
`starts_with()` | selects columns starting with a string
`ends_with()` | selects columns that end with a string
`contains()` | selects columns that contain a string
`matches()` | selects columns that match a regular expression
`num_ranges()` | selects columns that match a numerical range
`one_of()` | selects columns whose names match entries in a character vector
`everything()` | selects all columns
`last_col()` | selects last column; can include an offset.

Each of these can be very useful in a given scenario.

### Exercise 3.1.1a
> Select all of the variables that contain the letter e in their name.

```{r}

```


## 3.1.2 Filtering rows

`filter()` is the next verb we'll cover today, and is used to extract rows based on logical tests.

Like `select()`, its first argument is the data, followed by conditions for filtering data. For example, let's say we want to filter rows for cases in the "No Label" condition.

```{r}
filter(ps_data, condition == "No Label")
```

Or we could select observations from the "No Label" condition for kids 3 years old or younger:

```{r}
filter(ps_data, condition == "No Label" & age <= 3)
```

We can also filter for observations that meet one condition or another, using `|` for OR. Let's get observations for kids younger than 3 or in the no label condition

```{r}
filter(ps_data, condition == "Label" | age <= 3)
```

`dplyr` also has a few helper functions for more advanced filtering. One that is pretty useful is `between()`. Let's use it to get kids between ages 2.1 and 2.5:

```{r}
filter(ps_data, between(age, 2.1, 2.5))
```

### Exercise 3.2a 
> Get Kids between the ages of 3 and 4 using `filter()` and the `between()` helper function.

```{r}

```

### Exercise 3.2b
> Get Kids between ages of 3 and 4 using `filter()` *without* using the `between()` function.

```{r}

```

## 3.3. Pipes

Pipes come from the `magrittr` library are available when you load the `tidyverse` (_probably unnecessary sidenote: they're technically imported with `dplyr` when you call library(tidyverse)_). Pipes are a way to write strings of functions more easily, creating *pipelines*. They are extremely powerful and useful. A pipe looks like this:

`%>%`

You can enter a pipe with the shortcut CTRL+Shift+M for PC; CMD+Shift+M for Mac.

### 3.3.1 A quick side note about the term pipe

As mentioned above, a pipe in piping syntax is symbolized by `%>%`. However, another character is sometimes called a pipe, which is the vertical bar |, which we saw with logical tests above (| means or in logical statements).

### 3.3.2 The logic of piping syntax

The general idea of piping syntax, is that we have some function on the left hand and right hand side of the pipe. The function on the left side is evaluated, and then the **output** of that function is passed to the function on the right hand side of the pipe as the **first argument** of that (RHS) function. Let's start with a simple example. We'll get the sum of the `age` variable from the `ps_data`.

You can think of pipes as standing in for *then*.

```{r}
ps_data$age %>% # LHS is age vector from ps_data
  sum() # pass that to the sum function
```

As you can see, on the left hand side of the pipe `%>%`, we have the age vector from `ps_data`. On the right hand side, we  have the function sum(), so the piped syntax is basically saying *Take age from PS_data then get the sum*.

We can make this look even a little cleaner by using the `select()` function:

```{r}
ps_data %>% # take the data, then...
  select(age) %>%  # select age, then...
  sum() # take the sum
```

Notice that we entered age as an argument in select and it *looks* like the first argument. Looks can be deceiving; the first argument is actually `.data = ps_data`, but that is hidden from view when piping.

**Style Tip:**
It's typically considered good practice to not have more than one pipe per line.

Bad:
```{r }
ps_data %>% select(age) %>% sum() 
```

Good:
```{r }
ps_data %>%
  select(age) %>% 
  sum() 
```

### 3.3.3 Why use pipes?

The most important and most often mentioned reasons to use pipes are *cleanliness* (which I hear is next to *godliness*) and efficiency:

1. Cleaner code
    * This is nice, because it helps make your code more readable by other humans (including your future self).

Piped: 
```{r }
ps_data %>% # take the data, then...
  select(age) %>%  # select age, then...
  sum()
```

VS Nested:
```{r}
sum(select(ps_data, age), na.rm = TRUE)
```

2. Cleaner environment
    * When you use pipes, you have basically no reason to save objects from intermediary steps in your data wrangling / analysis workflow, because you can just pass output from function to function without saving it.
    * Finding object you're looking for is easier.
    * Auto complete (with tab) a little more efficient.

3. Efficiency
    * This is efficiency for you, the person doing the coding (not more efficient computing).
    * Naming objects is hard; piping means coming up with fewer names.
    
4. More error-proof
    * Because naming is hard, you might accidentally re-use a name and make an error.


### 3.3.4 A note about Scaling

The gains in cleanliness and efficiency scale with the complexity of what you're doing. 

Let's say, we wanted to take our PS_data, filter for observations from kids between 2.5 and 3.2, and then select just the subject id and age variables, and then get unique kids (using the `unique()` function on the subject id).

Without pipes, you'll either end up with some difficult to read code:
```{r, eval = FALSE}
unique(select(filter(ps_data, age > 2.5 | age < 3.2), age, subid))
```
or some throwaway objects:
```{r, eval = FALSE}
data_subset_age <- filter(ps_data, age > 2.5 | age > 3.2)

data_subset_age_ids <- select(data_subset_age, subid, age)

unique(data_subset_age_ids)
```
With pipes, we can avoid these issues:
```{r}
ps_data %>% # take the data, then...
  filter(age > 2.5 | age > 3.2) %>% # filter for kids between 2.5 and 3.2, then...
  select(subid, age) %>% # select subject id and centered age, then...
  unique() # get unique rows
```

See, so much easier to read, and not flooding our environment with clutter and not taxing our already taxed minds with having to come up with a bunch of names. And keep in mind this is just chaining a few of commands together; it really pays off as you do more and more complicated things.

### 3.3.5 Saving the output of your pipe

Keep in mind that, like everything in R, you have to tell R to save the output of your pipe using the `<-`.
```{r}
unique_filtered_data <- ps_data %>% # take the data, then...
  filter(age > 2.5 | age > 3.2) %>% # filter for kids between 2.5 and 3.2, then...
  select(subid, age) %>% # select subject id and centered age, then...
  unique() # get unique rows
```


### Exercise 3.3a
> Take the `ps_data` data set. Using select and filter, get the number correct for kids at least 4 years old (note: there are several ways to do this, but the sum() function may be helpful). The output of your pipe should be a single number.

```{r}

```

# 4. Diving deeper into dplyr

We gained a little bit of familiarity with `dplyr` above as an intro to the tidyverse. Now, we'll dive a little deeper and explore some of the other ways we can transform data using the grammar of data manipulation.

## 4.1 More on `select()`
Let's review what we learned using about selecting columns. We'll use the `starwars` dataset, which comes with `dplyr`.

### 4.1.1 basic selects

Recall that one easy way to select columns is to use `select()`, providing the data as the first argument (w/ or w/o pipes) and desired unquoted column names separated by a comma:

```{r}
starwars %>% 
  select(name, homeworld)
```

### 4.1.2 selecting a range

Recall that we can also select a range of variables, by name. Let's get everything from name to homeworld:

```{r}
starwars %>% 
  select(name:homeworld)
```

### 4.1.3 selecting with helper functions

The helper fucntions allow us to do even more advanced column selection. For example, we can get all of the columns that contain the string `"color"`:

```{r}
starwars %>% 
  select(contains("color"))
```

### 4.1.4 Re-arranging columns with select

I didn't mention this before, but we can also use select to re-arrange our columns. This is one of the main uses for the `everything()` helper function. Let's re-arrange things so that `homeworld` comes first, followed by `name`, then by the remainin columns

```{r}
starwars %>% 
  select(homeworld, name, everything())
```

### Exercise 4.1a
> Re-arrange the `starwars` columns such that all of the columns that start with h are at the beginning, followed by the remaining columns.

```{r}

```

### Exercise 4.2a
> Select name, species, hair_color, skin_color, and eye_color from the starwars data. Use at least one helper function. 

```{r}
```

## 4.2 More on `filter()`

Recall that `filter()` is sort of like the complement to select, and is for *filtering* the data for certain observations (rows). 

### 4.2.1 Filters with one condition

Filters can be relatively simple. For example, we could filter for starwars characters that are less than 100cm in height:

```{r}
starwars %>% 
  filter(height < 100)
```

Or we could do even more advanced filtering, such as by filter for observations 1 SD above the average height:

```{r}
starwars %>% 
  filter(height > mean(height, na.rm = TRUE) + sd(height, na.rm = TRUE)) # don't forget na.rm!
```

Speaking of NAs, let's see which characters have missing mass by filtering for rows where mass is NA.

```{r eval = FALSE}
starwars %>% 
  filter(mass == NA)
```

It give us an empty df. What happened? `NA` is a special case when it comes to logical filtering. A value can't equal NA, it *is* NA. This is a subtle distinction. I try to remember it by reminding myself that an unknown could be equal to anything, but it definitely *is* an unknown.

So, if we want to filter for NAs, we have to use `is.na()`, which returns TRUE if an entry is NA. Let's try to get characters with missing mass again:

```{r}
starwars %>% 
  filter(is.na(mass)) # note you wrap the variable in is.na()
```

To get the values that aren't missing, you have to put a `!` before `is.na()`, so it looks like `!is.na(variable)`. Let's get every observation that isn't mising on mass:

```{r}
starwars %>% 
  filter(!is.na(mass))
```

This works because `!` converts a logical value to its opposite:

```{r}
!TRUE
!FALSE
! 5 == 5
! 5 == 4
```

### 4.2.2 Filter with multiple conditions

Recall that we can combine conditions using `&` or `|`. For example, we could get observations 1 SD above the mean of height and below the mean of mass

```{r}
starwars %>% 
  filter(height > mean(height, na.rm = TRUE) + sd(height, na.rm = TRUE) &
         mass < mean(mass, na.rm = TRUE))
```

We could get observations either 1 SD above *or* below the mean of height too:

```{r}
  starwars %>% 
  filter(height > mean(height, na.rm = TRUE) + sd(height, na.rm = TRUE) |
         height < mean(height, na.rm = TRUE) + sd(height, na.rm = TRUE))

```

You can also filter based on character variables. For example, we could get ever character grey hair:

```{r}
starwars %>% 
  filter(hair_color == "grey")
```

Or characters that have grey or brown hair:

```{r}
starwars %>% 
  filter(hair_color == "grey" |
         hair_color == "brown")
```

Or characters that don't have brown hair:

```{r}
starwars %>% 
  filter(hair_color != "brown")
```

You may be wandering if there is some way to extract those observations that have "grey" listed as one of several colors.

It's a little tricky, but it can be done by using the `stringr` function `str_detect()`. Time permitting, we'll cover `stringr` in a bit more detail tomorrow, but `str_detect()` can be used to look for a string in an object; it returns a logical (TRUE or FALSE) based on whether or not the string is found in that object. It is a lot like `grep()` which we saw earlier, but has the string/data as its first argument (a la tidyverse convention). Let's use it in combination with `filter()` to get these salt & pepper starwars characters:

```{r}
starwars %>% 
  filter(str_detect(hair_color, "grey")) # first argument is string var,
                       # followed by pattern it looks for
```

Since `str_detect()` returns a logical, we can negate it with `!` to get the opposite (observations that don't contain the string `"grey"` in `hair_color`):

```{r}
starwars %>% 
  filter(!str_detect(hair_color, "grey"))
```

### Exercise 4.2a
> Starting with the starwars data frame, filter for characters thats birth year is at least 200. Select just their name, species, and birth_year.

```{r}
```

### Exercise 4.2b
> Starting with the starwars data frame, filter for characters that don't have blue in their eye color (hint: it could have more than one color) and print their name, eye_color, and species.

```{r}
```

### Exercise 4.2c
> Starting with the starwars data frame, filter for droids from Tatooine or humans from Naboo. Select just name, homeworld, species, hair, eye, and skin color and make sure the columns in that order.

```{r}
```

## 4.3 Transforming data with mutate() and transmute()

The next verbs from dplyr we'll discuss are `mutate()` and `transmute()`. `mutate()` is for adding columns to a dataframe. `transmute()` is for replacing columns in a dataframe. Let's start with `mutate()`

### 4.3.1 `mutate()`

Recall from last time that we can add columns to a dataframe in base R using `data$new_col <-`. For example, we could add height in meters to our starwars data:

```{r}
starwars$height_in_m = starwars$height/100

starwars 

# cleaning it back up
starwars <- starwars %>% 
  select(-height_in_m)
```

In the `tidyverse`, we use `mutate()` instead. `mutate()` requires data as its first argument, followed by a set of expressions defining new columns, separated by a comma. For example, we could calculate height in meters and millimeters:

```{r}
starwars %>% 
  mutate(height_in_mm = height*10, # height in millimeters
         height_in_m = height/100) # height in meters
```

`mutate()` performs each calculation in order, so you can use variables created earlier within the same `mutate` call in later operations. For example, let's z score height and mass (using the base R function `scale()`) and then average them together using the base R function `rowMeans()`:

```{r}
starwars %>% 
  mutate(height_z = scale(height),
         mass_z = scale(mass),
         height_mass_z = rowMeans(data.frame(height_z, mass_z), na.rm = TRUE))
```

As you can see, getting row means (i.e., mean for each row across a set of columns) can be a little clunky in `mutate()`. The `rowwise()` function is designed to make this easier. To use it, we call it before a `mutate()` call. Let's try it

```{r}
starwars %>% 
  rowwise() %>% 
  mutate(height_z = scale(height),
         mass_z = scale(mass),
         height_mass_z = mean(c(height_z, mass_z), na.rm = TRUE))
```

Oops, as you can see we got `NaN`, because it's trying to get z scores for each row, which are not defined. We would need to do two `mutate` calls and use the `rowwise()` function between them:

```{r}
starwars %>% 
  mutate(height_z = scale(height),
         mass_z = scale(mass)) %>% 
  rowwise() %>% 
  mutate(height_mass_z = mean(c(height_z, mass_z), na.rm = TRUE))
```

You can overwrite columns with `mutate()`. For example, gender is currently a character, but we could turn it using mutate to overwrite it:

```{r}
starwars %>%
  mutate(gender = factor(gender))
```

### 4.3.2 `transmute()`

`transmute()` is similar to mutate, but instead of returning the original data frame with the new (additional) columns, it returns just the new columns. For example, we could use `transmute` if we just wanted a dataframe of z scored height and mass:

```{r}
starwars %>% 
  transmute(height_z = scale(height),
         mass_z = scale(mass))
```

Note that this basically equivalent to doing a `mutate()` followed by a `select()` for the newly mutated columns.

### Exercise 4.3a
> Create a new variable called bmi that is each character's bmi. bmi = kg / m^2. Mass is already in kg units, but height is currently in cm units, so height will need to be converted (1m = 100cm). 

```{r}
```

### Exercise 4.3b
> This time, do the same thing, but use transmute so that the resulting dataframe has just name, height in meters, mass, and bmi (hint: you can always mutate or transmute a variable to be equal to itself)

```{r}
```

## 4.4 Grouping Data with `group_by()`

`group_by()` is another `dplyr` function that creates groups based on one or more variables in the data. This affects all kinds of things that you then do with the data, such as mutating. It is pretty simple to use. It requires data as its first argument, and the you name the variables (unquoted) to group by separated by a column.

For example, we could group our `starwars` data by characters' homeworld:

```{r}
starwars %>% 
  group_by(homeworld)
```

Okay, by the looks of it, nothing happened. But, if we paste it into the console, you can see that it did change something. It adds some meta-data saying that `homeworld` is a grouping factor, but that is it.

But, it has powerful effects on how other functions interact with our grouped dataset. Let's see what happens when we take the mean mass with and without grouping:

```{r}
starwars %>% 
  mutate(mean_mass = mean(mass, na.rm = TRUE)) %>% 
  select(homeworld, mean_mass, everything()) # re-arrange for easy viewing
```

Now, let's see what happens if we `group_by()` first:

```{r}
starwars %>% 
  group_by(homeworld) %>% 
  mutate(mean_mass = mean(mass, na.rm = TRUE)) %>% 
  select(homeworld, mean_mass, everything())
```

You can also group by multiple factors, which you can just add as comma-separated unquoted variable names. For example, let's group by homeworld and species and get the average mass:

```{r}
starwars %>% 
  group_by(homeworld, species) %>% 
  mutate(mean_mass = mean(mass, na.rm = TRUE)) %>% 
  select(homeworld, mean_mass, everything())
```

### Exercise 4.4a
> Create two new variables called `species_mean_mass` that is equal to the average mass per species, and `species_centered_mass` that is equal to each character's mass minus their species' mean (i.e., center mass within species). Select name, species, and all of the mass variables.

```{r}
```

## 4.5 Summarizing data with `summarize()`

The next `dplyr` verb we'll cover is `summarize()`, which is used to summarize across rows of a dataset. It, like all `tidyverse` functions, requires data as its first argument, and then you enter your summary formulas separated by commas; it looks pretty similar to `mutate()` and `transmute()`. The outcoming dataset will have **just the variables you summarized** and lose everything else.

Just like in `mutate()`, each expression is `new_var_name = EXPRESSION`.

Let's use summarize on the starwars dataset to get the mean mass across all observations:

```{r}
starwars %>% 
  summarize(mean_mass = mean(mass, na.rm = TRUE))
```

We can also add some more summary statistis. For example, let's get the `sd` and `n` as well:

```{r}
starwars %>% 
  summarize(mean_mass = mean(mass, na.rm = TRUE),
            sd_mass = sd(mass, na.rm = TRUE),
            n = n())
```

### 4.5.1 Combining `group_by()` and `summarize()`

`group_by()` and `summarize()` can be combined to get group-level statistics. This is a great way to make tables of descriptive stats in R or to create aggregated datasets for some purposes. 

To use these together, you just run `group_by()` followed by `summarize()` in a pipeline. Let's take a look at the mass statistics per species:

```{r}
starwars %>% 
  group_by(species) %>% # just add the group_by() call
  summarize(mean_mass = mean(mass, na.rm = TRUE),
            sd_mass = sd(mass, na.rm = TRUE),
            n = n())
```

Let's clean that up a bit by filtering out species with just 1 observation:


```{r}
starwars %>% 
  group_by(species) %>% # just add the group_by() call
  summarize(mean_mass = mean(mass, na.rm = TRUE),
            sd_mass = sd(mass, na.rm = TRUE),
            n = n()) %>% 
  filter(n > 1)

```

Of course you can use multiple groups in `group_by()` to get crosstabs:

```{r}
starwars %>% 
  group_by(species, gender) %>% # just change the group_by() call
  summarize(mean_mass = mean(mass, na.rm = TRUE),
            sd_mass = sd(mass, na.rm = TRUE),
            n = n()) %>% 
  filter(n > 1)
```

### Exercise 4.5a
>Returning to the `ps_data`, get the total number correct (call it `num_correct`), the total number of item/condition combinations (call it `num_trials`), and the proportion correct (call it `prop_correct`) for each condition and item using group_by and summarize.

```{r}
```


## 4.6 Brief Overview of Other dpyr verbs

I wanted to tell you quickly about some other useful `dplyr` verbs we don't have time to go into depth with, but that are useful to know about.

### 4.6.1 arrange()

Arrange can be used to re-arrange the rows of a dataset based on some variable(s) values. It requires the data, then each variable you want to arrange the data by separated by a comma.

For example, we can arrange the data by mass:

```{r}
starwars %>% 
  arrange(mass)
```

You can make it descending order by wrapping the variable in `desc()`:

```{r}
starwars %>% 
  arrange(desc(mass))
```

And, you can provide multiple variables to sort on, and it will do each in order:

```{r}
starwars %>% 
  arrange(height, mass)
```

If you tell it to arrange by a character variable, it will order them alphabetically:

```{r}
starwars %>% 
  arrange(name)
```

### 4.6.2 rename()

`rename()` can be used to rename variables. It requires data as its first argument, then the rename expressions in the form `new_name = old_name`. For example, we could rename name from the starwars data:

```{r}
starwars %>% 
  rename(char_name = name)
```

And, you can provide multiple renames:

```{r}
starwars %>% 
  rename(char_name = name,
         height_cm = height,
         mass_kg = mass)
```

### 4.6.3 *_join()

`dplyr` has some useful functions for joining datasets together based on a unique key (or set of key variables) to match on. For example, if we had a dataset called `lightsabers` that has various starwars characters' lightsaber colors:

```{r}
lightsabers <- data.frame(name = c("Luke Skywalker", 
                                   "Darth Vader", 
                                   "Obi-Wan Kenobi", 
                                   "Dooku"),
                          lightsaber_color = c("green", 
                                               "red", 
                                               "blue", 
                                               "red"))
```

We could join this to our starwars data using one of the join functions, such as `left_join()`, which keeps everythin in the left hand dataset (the first datatset in the arguments) and only records that match in the right hand dataset:

```{r}
left_join(starwars,
          lightsabers)
```

And let's see what happens if we reverse the order:

```{r}
left_join(lightsabers,
          starwars)
```


There are also more complicated joins that you can learn more about [in Ch. 13 of R for Data Science](https://r4ds.had.co.nz/relational-data.html)

### 4.6.4 distinct()

`distinct()` can be used to get distinct entries within a dataset. It requires data as its first argument, and then variables you want to be distinct. Let's start with just one variable, distinct homeworlds in starwars:

```{r}
starwars %>% 
  distinct(homeworld)
```

You can also get distinct combination by supplying more than 1 variable:

```{r}
starwars %>% 
  distinct(homeworld, species)
```

Note that by default it give us just the variables included in the `distinct()` call. We can change this by changing the `.keep_all` argument to TRUE:

```{r}
starwars %>% 
  distinct(homeworld, species, .keep_all = TRUE)
```

### 4.6.5 count()

`count()` is an easy way to get the count of a variable in a dataset. It's basically a shortcut of `group_by()` and `summarize(n = n())`. It requires data as its first argument, then the variables you want counts of. For example, let's get counts of species in starwars:

```{r}
starwars %>%
  count(species)
```

You can also get counts of combinations of variables by adding more variables, separated by a comma:

```{r}
starwars %>% 
  count(species, homeworld) %>% 
  arrange(desc(n))
```


### Exercise 4.6a
> Take the `ps_data`, make sure that it has only distinct kids (based on subid), but keep all of the variables. Put the dataset in order from most frequent to least frequent age.

```{r}
```
