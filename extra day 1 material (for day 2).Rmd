---
title: ''
author: "Cory Costello"
date: "9/23/2019"
output: html_document
---
# 4. Reading data into R

Now that we have the `rio` package and a basic idea of how packages work, let's use it to read in some data! If you're used to a GUI system like excel or SPSS, reading in data in R can be a little bit confusing at first. 

Reading in data generally has two slightly challenging aspects for new users:

1. You need to call a function that works with a particular data format (csv, txt, sav, etc.).

2. You need to tell R where to look.

We'll use `import()` from `rio`, which does the first part for us. We just call `import()` and it calls the right read function given the file's extension (`.csv`, `.txt`, `.sav`, `.xlsx`, etc.). 

## 4.1 Working Directories & File Paths 

On to challeng # 2. When R looks for a file, it has a starting point. This is called the *working directory*. The working directory that you're currently in is displayed in the console window and the files tab. You can also get it with the `getwd()` function:

```{r}
getwd()
```

For this tutorial, your working directory should be wherever you downloaded the materials. If you opened the .Rmd, you should be in the uoregon_r_bootcamp directory already. You can change your working directory with:

```{r eval = FALSE}
setwd("PATH")
```

It is considered bad practice to use `setwd()` and it typically will not work in an Rmd; you can see the author's rationale for that [here](https://groups.google.com/forum/#!topic/knitr/knM0VWoexT0).

My recommondation is to either:

1. Use an R project. This is a relatively foolproof way of doing things.    
2. Use .Rmds and always open Rstudio from the particular Rmd. This is a little less foolproof.
3. Use .Rmds and set the working directory in the console or through R Studio's GUI. This is the least foolproof.

1 is probably best practice. 2 can save some time but may not be worth it. 3 is too risky for my tastes. If you do need to set the working directory from R Studio's GUI, do the following:

Session > Set Working Directory > Choose Directory

You can choose the folder you want to work in. The code for setting the working directory will populate in the console. You can then copy/paste this into your code if you'd like.

## 4.2 Importing & Exporting data with rio

### 4.2.1 Importing Data
As I mentioned earlier, the `import()` function from `rio` really simplifies reading data into R. Let's see that first hand by reading in the `pragmatic_scales_data`, a csv file:

```{r}
ps_data <- import("pragmatic_scales_data.csv")

```

Let's say that `ps_data` were a `.sav` SPSS data file. In rio, this is no problem, it will call the right function to read in `.sav` files. Let's give it a try, reading in the `pragmatic_scales_data.csv` located in the `data` subdirectory of our current working directory:

```{r}
ps_data <- import("data/ps_data.sav")
```

Notice that all I had to change was where to look (telling it to go to the `data/` subdirectory and the file called `ps_data.csv`)

You can see all of the file formats `rio` works with by running `?import`.

### 4.2.2 Exporting Data

You can also use rio to export your data, saving it in any of the formats that it works with. This is really simple and works just like `import()`, but is called `export()`. For `export()`, you provide the R dataframe object you want to export, and the path/name for the new file. For example, let's say I want to export `ps_data` as an xlsx file and put it into the `/data` subdirectory. I could do that with export:

```{r}
export(ps_data,
       "data/ps_data.xlsx")
```

### Exercise 4.2a
> I made a mistake when creating this and left the datasets in the `uoregon_r_bootcamp` folder instead of putting them into the `/data` directory. Let's fix that. We already fixed ps_data, but now I want you to fix `another_data_set.csv`. First import the data as `another_df`:

```{r}
another_df <- import("another_data_set.csv")
```


### Exercise 4.2b
> Now I want you to export the data and save it into the `data/` directory. Make sure the name of the dataframe is `another_data_set`, and make sure you save it out as a csv.

```{r}
export(another_df,
       "data/another_data_set.csv")
```

### Exercise 4.2c
> One of my colleagues insists we send them a .sav file so that they can run the analyses in SPSS. Make another copy of `another_data_set` in the `data/` subdirectory that is in the .sav format. 

```{r}
another_df <- export(another_df, 
                     "data/another_data_set.sav")
```

### Exercise 4.2d
> Finally, let's read one of these datasets to make sure everything worked as expected. Import the .sav version of another_data_set as `another_df`.

```{r}
another_df <- import("data/another_data_set.sav")
```

## 4.3 Examining Your Data

Now that your data is in R, you may want to take a look at it. There are a few different ways to do that, which each offer different information.

### 4.3.1 View
There are a few different ways that we can see our data now that it is in R.

The most obvious way is to click on the View button in the environment pane. You should see ps_data in the environment pane with a little data icon at the far right. Click on that icon. You'll notice that this ran `View(ps_data)` in the console. We can do that with code:

```{r eval = FALSE}
View(ps_data)
```

Note that the V in `View()` is capital!.

### 4.3.2 head and tail

You can also see just the first few rows of a dataframe with `head()`:

```{r}
head(ps_data)
```

This can be useful when you have very large datasets as it is much faster than the View function. `head()` prints 6 rows by default, but you can increase or decrease that with the `n = ` argument. For example, imagine we want to see the first 20 rows:

```{r}
head(ps_data, n = 20)
```

tail is the complement to head, displaying just the final rows from a dataframe:

```{r}
tail(ps_data)
```

### 4.3.3 Examine Structure with str

We saw str a little earlier when we first introduced dataframes. It's worth mentioning it again because it can be so useful when you import data to see how the read function interpreted the variables. Let's see:

```{r}
str(ps_data)
```

### 4.3.4 Summary

The `summary()` funciton can be used to get a quick sense of each of the variables in a dataframe. It displays summary information for each variable. It displays different kinds of information depending on the variable's type.

```{r}
summary(ps_data)
```

# 5. More Advanced Indexing and Modifying a data frame in base R.

## 5.1 Indexing

Let's start by reviewing indexing dataframes.

### 5.1.1 Bracket Indexing with Numerical Indices and Names

Recall that uou can select entries in the data frame just like indexing a matrix, i.e., [row, column]

```{r}
ps_data[1, 5]
ps_data[1, "condition"]
```

And you can get a whole row or column by leaving the other dimension empty. Let's get all rows of condition:

```{r}
ps_data[, "condition"]
```
### 5.1.2 Indexing with $

Recall that you can also get a column from a df by using `df$column`. For example, we could get condition:

```{r}
ps_data$condition
```

## 5.1.3 Indexing with Logical Tests

Up to this point, we've only covered indexing by numerical index or name. But, you can also index via logical tests. To do this in base R, we use the `which()` function, which returns the indices where a condition is true. We can test if things are equal, not equal, greater, or lesser using the following symbols:

Test  | symbol
------|-------
Equal | ==
Not equal | !=
Greater than | >
Lesser than | <
Greater than or Equal to | >=
Lesser than or Equal to | <=

Let's put this to use and get all of the indices where condition is equal to label:

```{r}
which(ps_data$condition == "Label")
```

We can combine this with `[]` indexing to do even more powerful subsetting. For example, we can put this `which()` call within the row position to extract the rows for subjects in the "Label" condition. 

```{r}
ps_data[which(ps_data$condition == "Label"),]
```

Or, we could get all of the rows where subjects are greater than or equal to 2.5 years old:

```{r}
ps_data[which(ps_data$age >= 2.5),]

```

You can also use logical tests for columns, though that is a little trickier. Let's get all of the columns that start with the letter c. We can look for variables that start with c by using `str_detect()` on the column names, looking for entries that start with c `"^c"`.

```{r}
ps_data[,which(str_detect(colnames(ps_data), "^c"))]
```

You can also do more complicated logical tests by including `&` for AND and `|` for OR. For example, let's get subjects that were in the label condition and less than 3 years old:

```{r}
ps_data[which(ps_data$condition == "Label" & ps_data$age < 3),]
```

Or we might want subjects rows where the item is either faces or houses.

```{r}
ps_data[which(ps_data$item == "faces" | ps_data$item == "houses"),]
```


### Exercise 5.1a
> Get the first 10 rows of the item column from the `ps_data` df.

```{r}
ps_data[1:10, "item"]
```

### Exercise 5.1b
> Using logical indexing, get all of the rows where age is greater than or equal to 3.5 and item equals "faces".

```{r}
ps_data[which(ps_data$age >= 3.5 & ps_data$item == "faces"),]
```


### Exercise 5.1c
> Using logical indexing, get all of the columns that start with either s or a (Hint: you will need to use str_detect twice).

```{r}
ps_data[,which(str_detect(colnames(ps_data), "^s") |
                 str_detect(colnames(ps_data), "^a"))]
```

## 5.2 Modifying a Dataframe

We can apply functions to a columns within a dataframe to calculate new values. For example, let's get the mean of age for our practical scales data:

```{r eval = FALSE}
mean(age)
```

This doesn't work because there is nothing in the environment called age. There is an `age` column within the dataframe `ps_data`, but we have to tell R to look at the `ps_data` dataset:

```{r}
mean(ps_data$age)
```

### Exercise 5.2a
> Let's center age. Create a new column called age_centered in which you center age by subtracting the mean age from the age column (hint: you can create new columns using assignment: `df$new_col <- x`).

```{r}
ps_data$centered_age <- ps_data$age - mean(ps_data$age)
```

# 6. Introduction to the `tidyverse`

We installed and loaded the `tidyverse` earlier and now we'll learn some of the basics. ["The `tidyverse` is an opionated collection of R packages designed for data science"](https://www.tidyverse.org/). It's a suite of packages designed with a consistent philosophy and aesthetic. This is nice because all of the packages are designed to work well together, providing a consistent framework to do many of the most common tasks in R including:

* data cleaning (`tidyr`)
* data manipulating (`dplyr`)
* data visualization (`ggplot2`)
* working with strings (`stringr`)
* working with factors (`forcats`)

Among others. We'll be using functions from each of these packages. In fact, we already used one from `stringr`, when we used `str_detect()`.

Today we'll just take a brief tour of some aspects of the tidyverse. We'll spend the next two meetings going more in depth into tidyverse packages. 

Three qualities of the `tidyverse` are worth mentioning at the outset:

1. packages are designed to be like *grammars* for their task, so we'll be using terms like verbs to discuss the tidyverse. The idea is that you can string these grammatical elements together to form more complex statements, just like with language. 

2. The first argument of (basically) every function is data. This is very handy, especially when it comes to piping (discussed below).

3. Variable names are *usually* not quoted.

Without further ado, let's get started with dplyr:

## 6.1 dplyr

[`dplyr`](https://dplyr.tidyverse.org/) is a grammar of data manipulation. It is made up of several verbs for common data manipulation tasks, which we will go through briefly today and in more detail next time.

### 6.1.1 Selecting Columns

The `select()` is the first verb we'll cover and is how we can subset columns. If you're like me, you'll soon find it **much** easier to use than the bracket subsetting we did earlier. 

`select()` is the verb for selecting columns from a dataframe. The first argument is data followed which columns you would like to select.

#### 6.1.1.1 Basics of Select

You can indicate the columns you want to select using unquoted names. For example, let's select just `age` from `ps_data`

```{r}
select(ps_data, age)
```

You can select more columns by adding them, separated by a comma. Let's get age and condition:

```{r}
select(ps_data, age, condition)
```

You can also use columns' positions. We could get `subid`, the first column, by supplying a 1:

```{r}
select(ps_data, 1)
```

Or, you can say which variable you don't want by prefacing its name or index with a `-`. For example, let's get rid of age.

```{r}
select(ps_data, -age)
```

You could also get rid of by referencing its index:

```{r}
select(ps_data, -5)
```

You can also use `:` to select or de-select a range of variables. This can be done with reference to their numerical index:

```{r}
# select first three:
select(ps_data, 1:3)

# de-select last three:
select(ps_data, -(1:3)) # - requires parenthetical sequence
```

And you can even use ranges of variable names. 
```{r}
# select first three
select(ps_data, subid:correct)

# deselect first three
select(ps_data, -(subid:correct))
```

#### 6.1.1.2 Helper functions

The best part of select is that it has special helper function to perform common kinds of selection tasks.

##### starts_with
For example, let's say we want all the variables that start with 'c'. We can use the `starts_with()` helper function:

```{r}
select(ps_data, starts_with("c"))
```

That is way simpler than the base R solution we discussed above, which was

```{r}
ps_data[,which(str_detect(colnames(ps_data), "^c"))]
```

#### ends_with
Select columns that end with some character:
```{r}
select(ps_data, ends_with("e"))
```

#### contains
Select columns that contain a character.
```{r}
select(ps_data, contains("i"))
```

There are others too, but these are the most common. Here is a table of all of them.

function | what it does
---------|-------------
`starts_with()` | selects columns starting with a string
`ends_with()` | selects columns that end with a string
`contains()` | selects columns that contain a string
`matches()` | selects columns that match a regular expression
`num_ranges()` | selects columns that match a numerical range
`one_of()` | selects columns whose names match entries in a character vector
`everything()` | selects all columns
`last_col()` | selects last column; can include an offset.

Each of these can be very useful in a given scenario.

### Exercise 6.1.1a
> Select the two age variables (age and centered age) using one of the helper function.

```{r}
select(ps_data, contains("age"))
```


## 6.1.2 Filtering rows

`filter()` is the next verb we'll cover today, and is used to extract rows based on logical tests.

Like `select()`, its first argument is the data, followed by conditions for filtering data. For example, let's say we want to filter rows for cases in the "No Label" condition.

```{r}
filter(ps_data, condition == "No Label")
```

Or we could select observations from the "No Label" condition for kids 3 years old or younger:

```{r}
filter(ps_data, condition == "No Label" & age <= 3)
```

We can also filter for observations that meet one condition or another, using `|` for OR. Let's get observations for kids younger than 3 or in the no label condition

```{r}
filter(ps_data, condition == "Label" | age <= 3)
```

`dplyr` also has a few helper functions for more advanced things. One that is pretty useful is `between()`. Let's use it to get kids between ages 2.1 and 2.5:

```{r}
filter(ps_data, between(age, 2.1, 2.5))
```

### Exercise 6.2a 
> Get Kids between the ages of 3 and 4 using `filter()` and the `between()` helper function.

```{r}
filter(ps_data, between(age, 3, 4))
```

### Exercise 6.2b
> Get Kids between ages of 3 and 4 using `filter()` *without* using the `between()` function.

```{r}
filter(ps_data, age >= 3 & age <= 4)
```

## 6.3. Pipes

The `tidyverse` contains a package called `magrittr` which provides pipes. Pipes are a way to write strings of functions more easily, creating *pipelines*. They are extremely powerful and useful. A pipe looks like this:

`%>%`

You can enter a pipe with the shortcut CTRL+Shift+M for PC; CMD+Shift+M for Mac.

### 6.3.1 A quick sidenote about the term pipe

As mentioned above, a pipe in piping syntax is symbolized by `%>%`. However, another character is sometimes called a pipe, which is the vertical bar |, and this is used quite a bit in logical/boolean operations (| means or in logical statements).

### 6.3.2 The logic of piping syntax

The general idea of piping syntax, is that we have some function on the lefthand and righthand side of the pipe. The function on the leftside is evaluated, and then the **output** of that function is passed to the function on the righthand side of the pipe as the **first argument** of that (RHS) function. Let's start with a simple example. We'll get the mean of the `age` variable from the `ps_data`.

You can think of pipes as standing in for *then*.

```{r}
ps_data$age %>% # LHS is age vector from ps_data
  mean() # pass that to the mean function
```

As you can see, on the lefthand side of the pipe `%>%`, we have the age vector from `ps_data`. On the righthand side, we  have the function mean(), so the piped syntax is basically saying *Take age from ps_data then get the mean*.

We can make this look even a little cleaner by using the `select()` function:

```{r}
ps_data %>% # take the data, then...
  select(age) %>%  # select age, then...
  mean() # take the mean
```

Notice that we entered age as an argument in select and it *looks* like the first argument. Looks can be deceiving; the first argument is actually `.data = ps_data`, but that is hidden from view when piping.

**Style Tip:**
It's typically considered good practice to not have more than one pipe per line.

Bad:
```{r }
ps_data$age %>% mean() 
```

Good:
```{r }
ps_data$age %>%
  mean() 
```

### 6.3.3 Why use pipes?

The most important and most often mentioned reasons to use pipes are *cleanliness* (which I hear is next to *godliness*) and efficiency:

1. Cleaner code
    * This is nice, because it helps make your code more readable by other humans (including your future self).

Piped: 
```{r }
ps_data %>% # take the data, then...
  select(age) %>%  # select age, then...
  mean()
```

VS Nested:
```{r}
mean(select(ps_data, age), na.rm = TRUE)
```

2. Cleaner environment
    * When you use pipes, you have basically no reason to save objects from intermediary steps in your data wrangling / analysis workflow, because you can just pass output from function to function without saving it.
    * Finding object you're looking for is easier.
    * Autocomplete (with tab) a little more efficient.

3. Efficiency
    * This is efficiency for you, the person doing the coding (not more efficient computing).
    * Naming objects is hard; piping means coming up with fewer names.
    
4. More error-proof
    * Because naming is hard, you might accidentally re-use a name and make an error.


### 6.3.4 A note about Scaling

The gains in cleanliness and efficiency scale with the complexity of what you're doing. 

Let's say, we wanted to take our ps_data, filter for observations from kids between 2.5 and 3.2, and then select just the subject id and age variables, and then get unique kids (using the `unique()` function on the subject id).

Without pipes, you'll either end up with some difficult to read code:
```{r, eval = FALSE}
unique(select(filter(ps_data, age > 2.5 | age < 3.2), age, subid))
```
or some throwaway objects:
```{r, eval = FALSE}
data_subset_age <- filter(ps_data, age > 2.5 | age > 3.2)

data_subset_age_ids <- select(data_subset_age, centered_age, subid)

unique(data_subset_age_ids)
```
With pipes, we can avoid these issues:
```{r}
ps_data %>% # take the data, then...
  filter(age > 2.5 | age > 3.2) %>% # filter for kids between 2.5 and 3.2, then...
  select(subid, age) %>% # select subject id and centered age, then...
  unique() # get unique rows
```

See, so much easier to read, and not flooding our enviornment with clutter and not taxing our already taxed minds with having to come up with a bunch of names. And keep in mind this is just chaining a few of commands together. As we'll see next time, it really adds up when you have even more steps.

### 6.3.5 Saving the output of your pipe

Keep in mind that, like everything in R, you have to tell R to save the output of your pipe using the `<-`.
```{r}
unique_filtered_data <- ps_data %>% # take the data, then...
  filter(age > 2.5 | age > 3.2) %>% # filter for kids between 2.5 and 3.2, then...
  select(subid, age) %>% # select subject id and centered age, then...
  unique() # get unique rows
```


### Exercise 6.3a
> Take the `another_df` dataset. Using select and filter, get the number correct for kids at least 4 years old (note: there are several ways to do this, but the sum() function may be helpful). The output of your pipe should be a single number.

```{r}
another_df %>% 
  filter(age >= 4) %>% 
  select(correct) %>% 
  sum()

# or
another_df %>% 
  filter(age >= 4 & correct == 1) %>%
  nrow()
```

Okay, that's it for this time! Next time we'll dive deeper into the `tidyverse()`, learning the rest of the main verbs in `dplyr`, how to tidy data with `tidyr`, working with strings and factors (using `stringr` and `forcats`), and how to visualize data with ggplot2.