---
title: "Introduction to R"
author: "Jessica Kosie"
date: "9/20/2018"
output: 
  html_document:
    toc: true
    toc_float: true
---

# Goals 

By the end of this tutorial, you will know:

+ How to read in and examine data
+ How to get values out of the rows and columns in your data
+ What a pipe is and how to use pipes to chain together `tidyverse` verbs
+ Using tidyverse "verbs" to modifiy and summarise data
+ Intro to plotting in ggplot2 (more Friday)


The best way to do this tutorial is to walk through it slowly, executing each line and trying to understand what it does. You can execute a single line by placing your cursor somewhere in that line and hitting CMD+enter (again on a mac) and execute a whole chunk at a time by hitting CMD+option+C (on a mac). 

# 1. Quick Refresher: Reading data into R

First, you'll need to tell R where to look for the data. To do this, you will set your working directory. 

For this tutorial, your working directory should be wherever you downloaded the materials. (They should be on your desktop.)

```{r}

# What is my current working directory?
getwd()

# How can I change my working directory?
setwd("/Users/jessicakosie/Documents/School/Workshops/uoregon_r_bootcamp/")
getwd()

# What is in my working directory?
dir()

```

You can also do this using RStudio, via the graphical interface on the top:

Session > Set Working Directory > Choose Directory

You can choose the folder you want to work in. The code for setting the working directory will populate in the console. You can then copy/paste this into your code if you'd like.

```{r}
# Now, let's read in the pragmatic_scales_data CSV file and save it as an object called ps_data
ps_data <- read.csv("pragmatic_scales_data.csv", header = TRUE)

```

> ProTip: You can also read in data files generated by other programs like SPSS. There are often packages to help with this.

Here's how you could read in this file if it were an SPSS file. (The code below won't work because the file is a .csv)
```{r}
# Uncomment the line below to install the `foreign` package
# install.packages("foreign")
library(foreign)
ps_data_spss <- read.spss('pragmatic_scales_data.sav', to.data.frame = TRUE)

```

# Examining the data file

You should now see the dataset in your environment.

We can simply look at the data frame. We can also get a summary of the data. (these are all **functions** too!)

```{r}
# Look at the first few rows of the data.
head(ps_data)

# Look at the final few rows of the data.
tail(ps_data)

# You can also specify the number of rows you'd like to see
head(ps_data, n = 10)
tail(ps_data, n = 2)

# Get a summary of the data.
summary(ps_data)

# You can also `View` data, which shows an interactive "spreadsheet" view. You can also get this by clicking on the dataset in the environment.
View(ps_data)

# Note the capital `V`
view(ps_data) #this doesn't work!

# We can also look at the structure of the data.
str(ps_data)

# I can get the column names of the data.
colnames(ps_data)

```
# 2. Indexing and modifying a data frame.

You can select entries in the data frame just like indexing a vector. [row, column]

```{r}
# Get the entire `condition` column.
ps_data[,5]
ps_data[,"condition"]

# Get the entry in row 3, column 4.
ps_data[3, 4]

# Get the entry in row 4 of the item column
ps_data[4, "item"] 
```

> ProTip: Many people use this kind of selection to modify individual entries, like if you just want to correct a single mistake at a paticular point in the data frame. Be careful if you do this, as there will be nothing in the code that tells you that `4` is the *right* element to fix, you'll just have to trust that you got that number right. 
```{r}
# For example:
ps_data[4, "item"] <- "faces"

# Now that entry is changed to "faces"

# Change it back:
ps_data[4, "item"] <- "beds"

# You can do this with column names, too. Maybe I want to change the name of the `item` column to `image`.
colnames(ps_data)[2] <- "image"
colnames(ps_data)
head(ps_data)

# Change it back:
colnames(ps_data)[2] <- "item"

# You can also select an entire column using the $ operation.
ps_data$condition

# ...or ask for only the unique entries from that column
unique(ps_data$condition)

# ...or get the frequency of each unique value
table(ps_data$condition)

# You can also use `table` with multiple variables
table(ps_data$condition, ps_data$item)

# Create a new column from a current column(s).
ps_data$new <- ps_data$age + ps_data$correct
head(ps_data)

# Adding age and correct together really doesn't make sense. Let's delete that column.
ps_data$new <- NULL
head(ps_data)

```

We can apply functions to an entire column. For example, I can get the mean age for my entire sample.

Note that I have to include the data file in this argument, if I just say mean(age) I'll get an error.

```{r}
mean(age) # doesn't work! I don't  have anything in my environment called "age"

mean(ps_data$age)
```

> **Exercise 2a.** Let's center age. Create a new column called age_centered in which you center age by subtracting the mean age from the age column.

```{r}


```

# Using the `tidyverse`

Content adapted from: https://github.com/mcfrank/tidyverse-tutorial and https://github.com/jkosie/openscience_tutorial

  > tidyverse is a package that has to be installed and loaded before you can use any of its functions.
  
The functions we've been using so far have been in **base R** and don't require additional packages. To use the functions in the tidyverse packages the `tidyverse` package must first be installed and loaded. Tidyverse packages include tidyr, dplyr, ggplot2, and more - see here for more info: www.tidyverse.org.
  
If you haven't installed the package, you'll need to run this command once:

`install.packages("tidyverse")`

```{r}
# Load the package (tell R that you want to use its functions)
library("tidyverse")
```

We're going to reread the data now, using `read_csv`, which is the `tidyverse` version and works faster and better in a number of ways!

```{r}
ps_data <- read_csv("pragmatic_scales_data.csv")
```


# 3. Pipes

Pipes are a way to write strings of functions more easily. They bring the first argument of the function to the beginning. So you can write:

```{r}
mean(ps_data$age)

# becomes...

ps_data$age %>% mean()
```

That's not very useful yet, but when you start **nesting** functions, it gets better. 

```{r}
mean(unique(ps_data$age))
ps_data$age %>% unique() %>% mean()
ps_data$age %>% unique %>% mean
```

or 

```{r}
round(mean(unique(ps_data$age)), 
      digits = 2)

ps_data$age %>% unique %>% mean %>% round(digits = 2)

# indenting makes things even easier to read
ps_data$age %>%
  unique %>% 
  mean %>% 
  round(digits = 2)
```

This can be super helpful for writing strings of functions so that they are readable and distinct. 

> **Exercise 3a.** Rewrite these commands using pipes and check that they do the same thing! (Or at least produce the same output). 

Unpiped version: length(unique(ps_data$item))

```{r}


```
> **Exercise 3a. (cont.)** Let's try some code we used on Monday.

Unpiped version: plot(sin(seq(-pi,pi,.01)))

```{r}


```
# Using `tidyverse` to explore and characterize the dataset:

We are going to manipulate these data using "verbs" from `dplyr`. Here are four verbs that are common in many workflows (but there are many other useful ones):

+ `filter` - remove rows by some logical condition
+ `select` - choose only certain columns
+ `mutate` - create new columns 
+ `arrange` - changes the order of rows
+ `group_by` - group the data into subsets by some column
+ `summarise` - apply some function over columns in each group  

Inspect the various variables before you start any analysis. Earlier we used `summary` but it's not always very useful. 

```{r}
summary(ps_data)
```

This output just feels overwhelming and uninformative. 

You can look at each variable by itself:

```{r}
unique(ps_data$item)

ps_data$subid %>%
 unique 
```

Or use interactive tools like `View` or `DT::datatable` (which I really like).

```{r}
# this won't work unless you first do
# install.packages("DT")
DT::datatable(ps_data) 
```

> ProTip: What we're working with is called "tidy data" where each column is one measure, and each row is one observation. This is, by consensus, the best way to work with tabular data in R. It's actually where the name of `tidyverse` comes from. Check out [this paper](https://www.jstatsoft.org/article/view/v059i10) to learn more. BUT - if you normally work with "wide data", where each row is a subject and different trials are different columns (like what SPSS often does), you can get your data "tidy" using a package called `tidyr`, which is also part of the tidyverse. These verbs (`gather` and `spread`) can take a little more time to get used to, but we'll go over some basics today. 

## 4. Filtering 

There are lots of reasons you might want to remove *rows* from your dataset, including getting rid of outliers, selecting subpopulations, etc. `filter` is a verb (function) that takes a data frame as its first argument, and then as its second takes the **condition** you want to filter on. 

So if you wanted to look only at 2 and 3 year olds.

```{r}
ps_data %>%
  filter(age > 2, age < 3)

# filter(ps_data, age > 2 & age < 3)

```

Note that we're going to be using pipes with functions over data frames here. The way this works is that:

+ `dplyr` verbs always take the data frame as their first argument, and
+ because pipes pull out the first argument, the data frame just gets passed through successive operations
+ so you can read a pipe chain as "take this data frame and first do this, then do this, then do that."

This is essentially the huge insight of `dplyr`: you can chain verbs into readable and efficient sequences of operations over dataframes, provided 1) the verbs all have the same syntax (which they do) and 2) the data all have the same structure (which they do if they are tidy). 

OK, so filtering:

```{r}
#get only subjects between 2 and 3 years old
ps_data %>%
  filter(age > 2, 
         age < 3) 

#get all data for subject M3
ps_data %>% 
  filter(subid == "M3")

#get all data for participants NOT in the label condition
ps_data %>% 
  filter(condition != "Label")

```

**Exercise 4a.** Create a smaller datast with **only** the "faces" items in the "Label" condition.

```{r}


```

> ProTip: You can think about `filter`ing as similar to "logical indexing", where you use a vector of `TRUE` and `FALSE`s to get a part of a dataset, for example, `ps_data[ps_data$items == "faces",]`. This command creates a logical vector `ps_data$items == "faces"` and uses it as a condition for filtering.

A quick aside about logical operators:
```{r}

# == operator tests for equality

a = 3 #assigning a value of 3 to a (same as a <- 3)
a == 3 #does a equal 3? YES
a == 4 #does a equal 4? NO

a = 4 #if I do this
a == 4 #now a is equal to 4? YES
a == 3 #is a equal to 3? NO (because we changed the value we assigned to a)
a != 4 #is a NOT equal to 4? NO
a != 3 #is a NOT equal to 3? YES

# < and > (tip - the PacMan wants to eat whatever is bigger, e.g. 4>3 or 5<10)
# <= >= less than or equal to and greater than or equal to

a > 3 #yes, a = 4, which is greater than 3
a < 3 #no, 4 is not less than 3
a > 4 #no, 4 is not greater than 4
a >= 4 #but it is greater than or equal to 4! (because a = 4)
a <= 4 #...and it's less than or equal to 4! (because a = 4)

```
## 5. Selecting
There are also times when you want to add or remove *columns*. You might want to remove columns to simplify the dataset. If you wanted to do that, the verb is `select`. 

```{r}
#get ONLY the subid, age, and correct columns 
ps_data %>%
  select(subid, age, correct) 

#get everything EXCEPT the condition column
ps_data %>%
  select(-condition) 

#get the first column
ps_data %>%
  select(1) 

#get all columns whose name starts with the letters "co"
ps_data %>%
  select(starts_with("co")) 

```
## 6. Mutating
Perhaps more useful is *adding columns*. You might do this perhaps to compute some kind of derived variable. `mutate` is the verb for these situations - it allows you to add a column. Let's add a discrete age group factor to our dataset. 

```{r}
ps_data <- ps_data %>%
  mutate(age_group = cut(age, 2:5, include.lowest = TRUE))

head(ps_data)
```

**Exercise 5/6a.** Use tidyverse verbs to add a column called age_centered (age - mean(age)). Select only that new column and the subject IDs column. Do this WITHOUT re-saving your data file (i.e., these columns should show up in the console, but not be saved to ps_data). Hint: chain functions together using pipes!

```{r}


```
## 7. Arranging
Sometimes you might want to **sort** your data by the values in a particular column. For this, you'll use the verb `arrange`. Let's arrange our data by the `correct` column.

```{r}

ps_data %>% 
  arrange(correct)

# Correct == 0 are now the first entries in the data. Let's make the correct entries (correct == 1) come first! For this you'll use `desc`.
ps_data %>% 
  arrange(desc(correct))

# We can also sort by 2 columns
ps_data %>% 
  arrange(desc(correct), age)

```

**Exercise 7a.** Use `arrange` to find out the age of the **oldest** participant in the dataset.

```{r}


```
## 8. Standard psychological descriptives

We typically describe datasets at the level of subjects, not trials. We need two verbs to get a summary at the level of subjects: `group_by` and `summarise` (kiwi spelling). Grouping alone doesn't do much.

```{r}
ps_data %>%
  group_by(age_group) 
```

All it does is add a grouping marker. 

What `summarise` does is to *apply a function* to a part of the dataset to create a new summary dataset. So we can apply the function `mean` to the dataset and get the grand mean. 

```{r}
## DO NOT DO THIS!!!
# foo <- initialize_the_thing_being_bound()
# for (i in 1:length(unique(ps_data$item))) {
#   for (j in 1:length(unique(ps_data$condition))) {
#     this_data <- ps_data[ps_data$item == unique(ps_data$item)[i] & 
#                       ps_data$condition == unique(ps_data$condition)[n],]
#     do_a_thing(this_data)
#     bind_together_somehow(this_data)
#   }
# }

ps_data %>%
  summarise(correct = mean(correct))
```

Note the syntax here: `summarise` takes multiple  `new_column_name = function_to_be_applied_to_data(data_column)` entries in a list. Using this syntax, we can create more elaborate summary datasets also:

```{r}
ps_data %>%
  summarise(correct = mean(correct), 
            n_observations = length(subid))
```

Where these two verbs shine is in combination, though. Because `summarise` applies functions to columns in your *grouped data*, not just to the whole dataset!

So we can group by age or condition or whatever else we want and then carry out the same procedure, and all of a sudden we are doing something extremely useful!

```{r}
ps_means_nosubgrouping <- ps_data %>%
  group_by(age_group, condition) %>%
  summarise(correct_mean = mean(correct),
            correct_sd = sd(correct),
            n_observations = length(subid))
ps_means_nosubgrouping
```

> **Exercise 8a.** One of the most important analytic workflows for psychological data is to take some function (e.g., the mean) *for each participant* and then look at grand means and variability *across participant means*. This analytic workflow requires grouping, summarising, and then grouping again and summarising again! First, create a table called `ps_sub_means` to get the mean `correct` for each `subid`. Hint: in addition to `subid`, group by `age_group` and `condition` in this summary to transfer those values to `ps_sub_means` (we'll use them in our next step). Next, create a table called `ps_means` where you use `ps_sub_means` as your data file and calculate the mean `correct` across `age_group` and `condition`.  

```{r}



```
## 9. Data tidying
Credit: http://r4ds.had.co.nz/tidy-data.html#tidy-data-1

The two key verbs in **data tidying** are `spread` and `gather`. Each of these verbs relies on a key value pair that contains a *key* that indicates *what* the information describes and a *value* that contains the actual information.

"Password: 0123456789"" is a key value pair. 0123456789 is the *value*, and it is associated with the *key* Password.

The `ps_data` we've been using is already *tidy*. Now, we'll look at some that are not! We'll use some datasets that are built into the tidyverse package. Each dataset has the same values of four variables, `country`, `year`, `population`, and `cases`, but each dataset organizes them in different ways.

Recall the "rules" for tidy data:
+ Each variable must have its own column.
+ Each observation must have it's own row.
+ Each value must have its own cell.
```{r}
?table1 #let's learn about the datset
table1 #this dataset is tidy!
```
## 9. Gathering & Joining Data
A common problem is a dataset where some column names are not names of variables, but *values* of a variable. Check out `table4a`. The column names `1999` and `2000` are not variables in our data, instead they represent values of the `year` variable, and each row represents two observations, not one. We need to *gather* these columns into a new pair of variables.

*gather* makes wide tables narrower and longer
```{r}
table4a
```
There are three questions we must answer:
1. Which columns do we want to gather? A: `1999` and `2000`
2. What is the name ofthe variable whose values form the column names (the *key*)? A: year
3. What is the name of the variable that is spread out over the cells (the *value*)? A: cases

We'll use these answers in the *gather* function:
```{r}
table4a %>% 
  gather(`1999`, `2000`, key = "year", value = "cases")

```
> **Exercise 9a.** `table4b` contains information about the `population` variable. Let's *gather* that table as well. Type `table4b` to check it out before gathering. Your resulting table should have columns for `country`, `year`, and `population`.
```{r}
table4b #check it out


```
Note that table4a contained our `cases` data while table4b contained our `population` data. Everything else is the same. Let's join those two tables together using `left_join`. (see http://r4ds.had.co.nz/relational-data.html for more about joining data frames).
```{r}
tidy4a <- table4a %>% 
  gather(`1999`, `2000`, key = "year", value = "cases")

tidy4b <- table4b %>% 
  gather(`1999`, `2000`, key = "year", value = "population")

left_join(tidy4a, tidy4b) #note that R tells you which columns were matched

```
## 10. Spreading Data
In contrast to *gather*ing, sometimes a single observation is scattered across multiple rows. Then, you'd want to use *spread* (which is the opposite of *gather*). In table2, a single observation is a country in a year (`type` is not a variable they are interested in including in their analyses), but each observation is spread across two rows.

*spread* makes long tables shorter and wider
```{r}
table2

```
Now, there are two questions we must answer:
1. Which column contains the variable names (i.e., the *key* column)? A: `type`
2. Which column contains the *values* (or the data from multiple variables)? A: `count`

We'll use these answers in the *spread* function:
```{r}
table2 %>% 
  spread(key = type, value = count)
```
> **Exercise 10a.** Let's play around with our ps_data. Make each `item` a unique variable. Use *spread* to reformat the data so that there is a unique column for each item. The values in each of the four `item` columns should indicate whether or not the subject got that particular item right or wrong (i.e., `correct` in ps_data). Hint: what is the *key*? What is the *value*? Do not save this as a new object.
```{r}


```
## 11. Graphing in ggplot2
For this section we're going to use another dataset that is built into R. It is called `iris`. Let's start by making a scatter plot of the relationship between Sepal.Length and Petal.Length.

Note: this is just the beginning! There are entire books on graphing in ggplot2! https://www.amazon.com/ggplot2-Elegant-Graphics-Data-Analysis/dp/331924275X/ref=as_li_ss_tl?ie=UTF8&linkCode=sl1&tag=ggplot2-20&linkId=4b4de5146fdafd09b8035e8aa656f300
```{r}
?iris #first, let's learn about the dataset

ggplot(data = iris) +
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length))

#this is the ggplot version of:
plot(iris$Sepal.Length, iris$Petal.Length)
```
Creation of a plot in ggplot2 begins with the function ggplot() which creates a coordinate system you can add data to. For example:
```{r}
ggplot(iris)
```
You then can add one or more layers to ggplot() to complete your graph. As above, we'll add a layer of points to our plot (creating a scatterplot). geom_point() is a *geom function*, each of which adds a different type of layer to a plot. Each *geom function* take a *mapping* argument that defines visual properties of the graph. 

Template for graphing in ggplot2:

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))
  
```{r}
ggplot(data = iris) +
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length))
```
Let's find out if the relationship between `Sepal.Length` & `Petal.Length` relates to the `Species` of iris. To visualize this, we are going to map `Species` to an *aesthetic* (or a visual property of one of the objects in our plot). Aesthetics include things like size, shape, or color of our points. Let's map the color of our points to the `Species` variable.
```{r}
ggplot(data = iris) +
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length, color = Species))

#ggplot assigned each level of *color* to each unique value of `Species`. This is called *scaling*.
```
> **Exercise 11a.** Options for *aesthetics* include color, shape, size, and alpha. Create a scatter plot to visualize the relationship between `Sepal.Width` and `Petal.Width`. Add an aesthetic to visualize the effect of `Species`. Choose any aesthetic you'd like or play around with a few. What do they do? How might you use more than one aesthetic?
```{r}


```

> ProTip: We could also make separate graphs for each `Species` using `facet_wrap`. By passing a formula (data structure) to `facet_wrap`.

Here's how:
```{r}
ggplot(data = iris) +
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  facet_wrap(~ Species)
```
Maybe we'd prefer a line graph instead of a scatterplot to describe the `iris` data. In this case, we'd use a different *geom* (e.g., point, line, smooth, boxplot, bar). 

Let's make the same plot, using the smooth *geom* which fits a smoothed line to the data.
```{r}
ggplot(data = iris) +
  geom_smooth(mapping = aes(x = Sepal.Length, y = Petal.Length))

#loess is the default function for geom_smooth() 
# from http://www.statisticshowto.com/lowess-smoothing/: LOWESS (Locally Weighted Scatterplot Smoothing), sometimes called LOESS (locally weighted smoothing), is a popular tool used in regression analysis that creates a smooth line through a timeplot or scatter plot to help you to see relationship between variables and foresee trends.
```
We can add aesthetics to help visualize the data:
```{r}
ggplot(data = iris) +
  geom_smooth(mapping = aes(x = Sepal.Length, y = Petal.Length, linetype = Species))

ggplot(data = iris) +
  geom_smooth(mapping = aes(x = Sepal.Length, y = Petal.Length, linetype = Species, color = Species))
```
We can also layer multiple geoms in the same plot!
```{r}
ggplot(data = iris) +
  geom_point(mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  geom_smooth(mapping = aes(x = Sepal.Length, y = Petal.Length))

#we could also have used global mappings
ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  geom_point() +
  geom_smooth()

#finally, we can add an *aesthetic* to only one part of the graph
ggplot(data = iris, mapping = aes(x = Sepal.Length, y = Petal.Length)) +
  geom_point(mapping = aes(color = Species)) +
  geom_smooth()
```
> **Exercise 11b.** Plot the relationship between `Sepal.Width` and `Petal.Width`. As above, create lines overlaid on a scatter plot. For the points, use different colors for each `Species`. For the lines, use both different colors and line types for `Species`.
```{r}


```
Now let's try a bar plot. The bar chart below displays the total number of irises in the `iris` dataset, grouped by `Species`.
```{r}
ggplot(iris, aes(x = Species)) +
  geom_bar()
# not very interesting - it looks like there's an approximately equal number of irises for each species; note that geom_bar defaults to *count*. This would be more informative if we used another dataset that is built in to ggplot2 called `diamonds`.

ggplot(diamonds, aes(x = cut)) +
  geom_bar()
#this data set contains information about diamonds, and there are different numbers of diamonds at each level of `cut`
```
Let's get a more interesting plot for our `iris` dataset. We can look at the avearge `Sepal.Length` for each `Species` of iris.

First, we'll need to use the skills we've previously acquired to create a table of means that we'll then plot. We'll then use the argument stat = "identity" to map the mean value onto the y aesthetic.
```{r}
plot.data <- iris %>% 
  group_by(Species) %>% 
  summarise(mean = mean(Sepal.Length, na.rm=TRUE))

ggplot(plot.data, aes(x=Species, y=mean)) +
  geom_bar(stat="identity")

#more options:
ggplot(plot.data, aes(x=Species, y=mean, color=Species)) +
  geom_bar(stat="identity")

ggplot(plot.data, aes(x=Species, y=mean, fill=Species)) +
  geom_bar(stat="identity")
```
> **Exercise 11c.** Now, let's make a plot for ps_data. We want to visualize the mean `correct` for each `item` across each level of `condition`. Make sure the color of the bars represents the different `item`s. Instead of putting both levels of `condition` on the same plot, create two separate plots, one for each `condition`. Hint: use what you've learned so far about grouping and summarising; see *facet_wrap* above for help separating plots by `condition`!
```{r}


```